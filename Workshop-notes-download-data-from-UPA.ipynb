{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import wasabisql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source of Truth Loader\n",
    "Me permite traer todos los archivos .avro en los que están distribuidos los datos de navegación de UPA entre dos fechas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIP 1: Empezá con small data\n",
    "Para armar un script empezá trayendo una hora. Cuando el script entero funcione, traé el resto de los datos que necesites. Esto es porque cuanto más datos pido, más tarda en procesar, así que empezar por pocos datos agiliza el proceso de escribir el script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIP 2: Bajar datos en tandas\n",
    "A Spark le cuesta trabajar con demasiados datos avro. Cada día de datos se compone de unos ~6 archivos. Si necesitás más de una semana de datos, bajá en tandas de a una semana y luego uní. Si no, hacen paro los python workers.\n",
    "http://stackoverflow.com/questions/34461804/stackoverflow-due-to-long-rdd-lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader = wasabisql.SOTLoader(sqlContext)\n",
    "\n",
    "# Los inputs de la función getEvents son una fecha de inicio, una de fin, y los campos que querés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime(2016, 6, 12, 5)\n",
    "end = datetime.datetime(2016, 6, 12, 6)\n",
    "campos = ['userid', 'datetime', 'pr', 'fl', 'cc', 'ci', 'co', 'dc', 'hr', 'hid', 'di','hc', 'pri', \n",
    "                           'pritax', 'cur', 'exch']\n",
    "\n",
    "events = loader.getEvents(campos, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### events es un DataFrame de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Está organizado en columnas y está distribuido en partecitas en diferentes máquinas). Un Dataframe tiene adentro un RDD y un schema para organizar ese RDD. \n",
    "A data frame is a table, or two-dimensional array-like structure, in which each column contains measurements on one variable, and each row contains one case. So, a DataFrame has additional metadata due to its tabular format, which allows Spark to run certain optimizations on the finalized query.\n",
    "An RDD, on the other hand, is merely a Resilient Distributed Dataset that is more of a blackbox of data that cannot be optimized as the operations that can be performed against it are not as constrained. Also, RDD is immutable.\n",
    "However, you can go from a DataFrame to an RDD via its rdd method, and you can go from an RDD to a DataFrame (if the RDD is in a tabular format) via the toDF method\n",
    "In general it is recommended to use a DataFrame where possible due to the built in query optimization.\n",
    "\n",
    "source: http://stackoverflow.com/questions/31508083/difference-between-dataframe-and-rdd-in-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIP 3: Filtrar todo lo que no necesitás lo antes posible\n",
    "Así no estás procesando datos que nunca terminás usando y que vuelven más lentos tus procesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar con condiciones tipo SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aplicar filtros sobre cada columna\n",
    "events = events.filter(\"length(userid) = 36\")\\ #me quedo solo con acciones donde el userid respeta el formato habitual\n",
    "                .filter('lower(fl) in (\"search\", \"detail\", \"checkout\", \"thanks\")')\\ #elijo solo determinados flows\n",
    "                .filter('fl not like \"ab%\"')\\ # Me quedo con las acciones donde el flow no empieza en \"ab\" \n",
    "                .filter('fl != \"event-suscription\"') # Excluyo acciones donde el flow sea event-subscription\n",
    "                .filter('lower(pr) in (\"hotels\")')\\ # Me quedo solo con acciones donde el producto es hotels\n",
    "                .filter('lower(cc) in (\"mx\")')\\ # Me quedo solo con acciones del site MX\n",
    "                .filter(\"dc is not null\") # Me quedo solo con acciones donde el campo dc no está vacío"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Puedo elegir quedarme con solo algunos campos\n",
    "campos_sin_pr = ['userid', 'datetime', 'fl', 'cc', 'ci', 'co', 'dc', 'hr', 'hid', 'di','hc', 'pri', \n",
    "                           'pritax', 'cur', 'exch']                \n",
    "events = events[campos_sin_pr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unir los DF (si hice mi getEvents por partes)\n",
    "Si solo son dos Dataframes los que tengo que unir, simplemente puedo hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = eventsA.unionAll(eventsB)\n",
    "# Si son varios Dataframes los que tengo que unir puedo hacer esta funcion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allevents = allunionAll(eventsA, eventsB, eventsC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo bajar muchos días de datos de forma automatizada\n",
    "En vez de hacer  \"events = loader.getEvents(campos, start, end)\" una y otra vez cambiando start y end para después unir todos esos dataframes con unionAll, se puede hacer esto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) hacés una lista vacía a la cual vas a ir appendeando los dataframes de events de cada día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allevents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Determinás que campos vas a pedir y el inicio y fin del período\n",
    "(la fecha de fin no entra en el período así que sumar un día)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campos = ['userid', 'datetime', 'pr', 'fl', 'cc', 'ci', 'co', 'dc', 'hr', 'hid', 'di','hc', 'pri', \n",
    "                               'pritax', 'cur', 'exch']\n",
    "\n",
    "start_range = datetime.datetime(2016, 6, 12, 0)\n",
    "end_range = datetime.datetime(2016, 6, 19, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se necesitan dos funciones adicionales: \n",
    "- generateDays, que genera los rangos de días para que loader traiga de a uno.\n",
    "- getEventsDay que usa getEvents para bajar los eventos de ese día, aplica el filtro, crea un DataFrame por día y hace un checkpoint para cada dataframe (que contiene un día de eventos)\n",
    "\n",
    "(!) El checkpoint es importante porque cuando llega el momento de la acción (un count, take, etc.) para procesarla Spark arrastra la información de todas las transformaciones que hubo antes. El checkpoint borra toda esa historia previa y se queda solo con el DataFrame, haciendo que sea más fácil de procesar. \n",
    " \n",
    "En este caso, el checkpoint está metido dentro de la función que busca los eventos de cada día, getEventsDay, así se hacen mini checkpoints por día antes de appendear ese DataFrame a la lista vacía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateDays(start, end):\n",
    "    one_day = datetime.timedelta(1)\n",
    "    i = 0\n",
    "    while start + one_day*i < end:\n",
    "        yield start + one_day*i\n",
    "        i+=1\n",
    "        \n",
    "def getEventsDay(start, end):\n",
    "    events = loader.getEvents(campos, start, end)\n",
    "    events = events.filter(\"length(userid) = 36\")                    .filter('lower(fl) in (\"search\", \"detail\", \"checkout\", \"thanks\")')                    .filter('lower(pr) in (\"hotels\")')                    .filter('lower(cc) in (\"mx\")')                    .filter(\"dc is not null\")\n",
    "    events = checkpoint(events)\n",
    "    events.count() # este count es solo para aplicar una acción que corra las transformaciones (el filter)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for start in generateDays(start_range, end_range):\n",
    "    end = start + datetime.timedelta(1)\n",
    "    events = getEventsDay(start, end)\n",
    "    allevents.append(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Unís la lista de DF en un solo DF\n",
    "Si pedís más de una semana de datos, hacerlo en diferentes Dataframes eventsA, eventsB, eventsC. Luego unir con está función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = reduce(DataFrame.unionAll, allevents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar los datos\n",
    "Si bajaste una gran cantidad de días, y eso te llevó horas, y vas a seguir usando el DF por varios días, y ya está lo suficientemente filtrado, es una buena idea guardarlo en el storage de p13n. \n",
    " \n",
    "(!) En el storage de p13n no se guardan porquerías y en lo posible tampoco archivos demasiado chicos. Esto es porque sí o sí el sistema de archivos está configurado para guardar lo que sea en varios archivos de 128Mb entonces aún con un archivo chico, las porquerías llenan el disco rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# se guarda en la carpeta /dataset\n",
    "# poner un nombre representativo de lo que contiene\n",
    "# usar formato parquet\n",
    "# si vas a guardar varias veces el mismo archivo actualizandolo, poner Overwrite para que no salte un error\n",
    "events.write.option(\"header\", \"true\").save('dataset/DynFeeEvents28d-MX.parquet', format=\"parquet\", mode=\"Overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Después el DF guardado por Spark (es distinto si lo guardaste con Pandas!) se levanta así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = sqlContext.read.load(\"dataset/DynFeeEvents28d-MX.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupar por usuarios\n",
    "Puedo hacer esto cuando quiero sacar diferentes métricas para un mismo usuario. Ej: cuántas sesiones tuvo, cuántos destinos distintos vio.\n",
    "También puedo hacerlo cuando quiero quedarme con sólo una acción relevante para cada usuario.\n",
    " \n",
    "Partimos de un dataframe de Spark que estaba distribuido en partecitas en diferentes máquinas. El resultado de group by es un rdd distribuido donde los distintos eventos de un mismo usuario quedan guardados en la misma máquina. Esto hace que todos los procesos/funciones que aplique de acá en adelante sean más eficientes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = events_rdd.groupBy(lambda x: x.userid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
